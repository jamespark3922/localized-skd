"""
 Copyright (c) 2022, salesforce.com, inc.
 All rights reserved.
 SPDX-License-Identifier: BSD-3-Clause
 For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause
"""

import os
import json
from tqdm import tqdm 
import numpy as np
import pandas as pd
import io
from typing import List, Dict
import random
import torch
from PIL import Image
from PIL import ImageFile

ImageFile.LOAD_TRUNCATED_IMAGES = True

from lavis.datasets.datasets.base_dataset import BaseDataset
from lavis.datasets.datasets.sherlock_datasets import crop_widescreens
from lavis.datasets.vc_utils import  draw_region, add_tags, tokens2str, get_references, read_jsonl

"""
    {
        'qa': 'Question: What is the engine of the plane suggesting about the scene? Answer: The engine of the plane suggests that this is an active and exciting scene, possibly an air show or aerobatics event. ',
        'qar': {
            'question': 'What is the engine of the plane suggesting about the scene?',
            'answer': 'The engine of the plane suggests that this is an active and exciting scene, possibly an air show or aerobatics event. ',
            'rationale': 'rationale',
        }
        'index': 'chatgpt_region1_ref0_1',
        'region': '0',
        'image': 'VG_100K_2/2378381.jpg',
        'references': [
            {'name': '0',
            'boxes': [[114.4206848145, 171.4768066406, 136.3255767822, 193.6623840332]],
            'segms': None,
            'obj_guess': 'propeller',
            'tag': 'region'},
            {'name': '1',
            'boxes': [[109.6865615845, 179.2686004639, 129.4137573242, 202.239730835]],
            'segms': None,
            'obj_guess': 'motor',
            'tag': 'region'},
            {'name': '2',
            'boxes': [[114.892616272, 168.8507995605, 138.860748291, 185.9940795898]],
            'segms': None,
            'obj_guess': 'tank_(storage_vessel)',
            'tag': 'region'},
            {'name': '3',
            'boxes': [[152.9057159424, 176.6907348633, 162.3196868896, 185.0597229004]],
            'segms': None,
            'obj_guess': 'machine_gun',
            'tag': 'region'}
        ]
    }
"""
class UnifiedVQAChatGPTDataset(BaseDataset):
    """ UnifiedVQA format generated by ChatGPT
    """
    def __init__(self, vis_processor, text_processor, vis_root, ann_paths, info):        
        super().__init__(vis_processor, text_processor, vis_root, [])
        
        self.annotation = []
        self.debug = info.get('debug', False)
        for ann_path in ann_paths:
            data = []
            with open(ann_path) as f:      
                for line in tqdm(f): 
                    data.append(json.loads(line.strip()))
                    if self.debug and len(data) == 60000:
                        break
                self.annotation.extend(data)

        self.is_train = True
        self.region_mode = info.get('region_mode', 'boxes')
        self.draw_others = info.get('draw_others', False)
        self.use_multi_class = info.get('multi_class', False) # multi_class: class determined by number of accepts
        self.use_generative_label = info.get('use_generative', True)   # label as string; for blip-2 / t5 style model
        self.include_rationale = info.get('include_rationale', True)
        
        self.subset = info.get('subset', None)  # random subset
        if self.subset is not None:
            self.annotation = random.sample(self.annotation, int(self.subset))


        verbalizations = pd.read_json(info['region_mapping'], lines=True).to_dict(orient='records')
        self.regions = {d['image']: d['region_locations'] for d in verbalizations}
        # breakpoint()
        print(self.__getitem__(0))
         
    def get_regions(self, datum) -> List[Dict]:
        '''
        Get regions sorted in the order to draw the image.
        Retunrs something like this if the two regions are swapped.
            [
                {
                    'boxes': [[754.598449707, 377.0939331055, 1483.8640136719, 1067.4243164062]], 
                    'obj_guess': 'person', 
                    'name': '1', 
                    'idx': 1, 
                },

                {
                    'boxes': [[420.2572021484, 341.606048584, 713.0709228516, 1061.1995849609]], 
                    'obj_guess': 'person', 
                    'name': '0', 
                    'idx': 0,
                }
            ]
        '''
        regions = datum['region']
        if self.is_train:  # apply region swapping augmentation in training
            # if random.random() > 0.5:
            if random.random() > 0.5:
                regions = random.sample(datum['region'], len(datum['region']))
            else:
                regions = random.sample(datum['region_all'], len(datum['region_all']))
        regions_info = [self.regions[datum['image']][ref] | {'name': ref} for ref in regions]   # List of bounding boxes and ids.
        return regions_info

    def load_region_image(self, datum, regions):
        # if 'VG_100K' in datum['image'] or 'vcr1images' in datum['image']:
        image_path = os.path.join(self.vis_root, datum['image']) 
        # else:
        #     image_path = os.path.join(self.vis_root, 'vcr1images', datum['image'])
        image = Image.open(image_path).convert('RGB')

        # assign color codeÂ based on the order they appear.
        for ref_idx, region in enumerate(regions):
            box = region['boxes'][0]
            image = draw_region(image, box, ref_idx, mode='boxes')
        return image

    def parse_region_tokens(self, tokens: List[str], regions: List[Dict]) -> str:
        """ 
        Replace region IDs in tokens in the order of `regions` with 0-index and get the string.

        :param: tokens (List[str]): List of tokens to replace the region IDs
        :param: regions (List[Dict]): list of regions containing bounding box coordinates and the IDs.
        """

        region_dct = {}
        for idx, region in enumerate(regions):
            region_dct[int(region['name'])] = region | {'index': idx}  # 'index' allows to keep consistent reference to the colored regions in `load_region_image``
 
        process_text = tokens2str(add_tags(tokens, region_dct, reorder_ids=True))

        return process_text
    
    def __getitem__(self, index, save_image=False):
        datum = self.annotation[index]

        regions = self.get_regions(datum)        

        image = self.load_region_image(datum, regions)        
        
        if save_image:  # debugging purpose...
            image.save('region.jpg') 
        image = self.vis_processor(image)   

        if 'text_input' in datum:
            text_input = self.text_processor(self.parse_region_tokens(datum['text_input'], regions))
            text_output = self.text_processor(self.parse_region_tokens(datum['text_output'], regions))
            full_text_input = text_output
        else:
            if 'full_text_input' in datum:
                full_text_input = datum['full_text_input']
            else:
                full_text_input = datum['question'] + ['Answer:'] + datum['answer'] + ['Rationale:'] + datum['rationale']
            full_text_input = self.parse_region_tokens(full_text_input, regions)
            
            if True:
                input_tokens = full_text_input.split()
                split_len = random.randint(1, (len(input_tokens) // 2))
                text_input = ' '.join(input_tokens[:split_len])
                text_output = ' '.join(input_tokens[split_len:])
            else:
                text_input = self.parse_region_tokens(datum['question'] + ['Answer:'], regions)
                text_output = self.parse_region_tokens(datum['answer'] + ['Rationale:'] + datum['rationale'], regions)

            text_input = self.text_processor(text_input)
            text_output = self.text_processor(text_output, add_prompt=False)

            full_text_input = self.text_processor(full_text_input)

        return {
                "image": image, 
                "text_input": text_input,
                "full_text_input": full_text_input,
                "text_output": text_output, 
                'instance_id': datum['index'],
                "image_id": index
            }
    
    def collater(self, samples):
        image_list, text_input, full_text_input, text_output, instance_id_list, image_id_list = [], [], [], [], [], []

        for sample in samples:
            image_list.append(sample["image"])
            text_input.append(sample["text_input"])
            full_text_input.append(sample["full_text_input"])
            text_output.append(sample["text_output"])
            instance_id_list.append(sample["instance_id"])
            image_id_list.append(sample["image_id"])

        return {
            "image": torch.stack(image_list, dim=0),
            "text_input": text_input,
            "full_text_input": full_text_input,
            "text_output": text_output,
            "instance_id": instance_id_list,
            "image_id": torch.tensor(image_id_list, dtype=torch.int),
        }

class UnifiedVQAChatGPTEvalDataset(UnifiedVQAChatGPTDataset, BaseDataset):
    """ UnifiedVQA Binary Label Classification Dataset
    """
    def __init__(self, vis_processor, text_processor, vis_root, ann_paths, info):        
        BaseDataset.__init__(self, vis_processor, text_processor, vis_root, [])

        self.annotation = []
        self.debug = info.get('debug', False)
        for ann_path in ann_paths:
            data = []
            with open(ann_path) as f:      
                for line in tqdm(f): 
                    data.append(json.loads(line.strip()))
                    # if self.debug and len(data) == 20:
                    #     break
                    if len(data) == 2000:
                        break
                self.annotation.extend(data)
        
        self.is_train = False
        self.region_mode = info.get('region_mode', 'boxes')
        self.draw_others = info.get('draw_others', False)
        self.use_multi_class = info.get('multi_class', False) # multi_class: class determined by number of accepts
        self.use_generative_label = info.get('use_generative', True)   # label as string; for blip-2 / t5 style model
        self.include_rationale = info.get('include_rationale', True)
        self.generate_question = info.get('generate_question', False)

        verbalizations = pd.read_json(info['region_mapping'], lines=True).to_dict(orient='records')
        self.regions = {d['image']: d['region_locations'] if 'region_locations' in d else d for d in verbalizations}
 
    def __getitem__(self, index):
        datum = self.annotation[index]

        regions = self.get_regions(datum)
        
        image = self.load_region_image(datum, regions)     
        image = self.vis_processor(image)   

        question = self.parse_region_tokens(datum['question'], regions)
        answer = self.parse_region_tokens(datum['answer'], regions)
        rationale = self.parse_region_tokens(datum['rationale'], regions)
        
        text_input = self.text_processor(f"{question} Answer:")
        text_output = self.text_processor(f"{answer} Rationale: {rationale}", add_prompt=False)
        full_text_input = self.text_processor("{} Answer: {} Rationale: {}".format(question, answer, rationale))
 

        return {
                "image": image, 
                "text_input": text_input,
                "full_text_input": full_text_input,
                "text_output": text_output, 
                'instance_id': datum['index'],
                "image_id": index
            }

class UnifiedVQAChatGPTContrastiveDataset(UnifiedVQAChatGPTDataset):
    """ 
        UnifiedVQA Contrastive Training for Data Generated by ChatGPT.
        This is used to train BLIP2 Model.
    """

    def __init__(self, vis_processor, text_processor, vis_root, ann_paths, info):        
        super().__init__(vis_processor, text_processor, vis_root, ann_paths, info)
        
    def __getitem__(self, index, save_image=False):
        datum = self.annotation[index]
        
        regions = self.get_regions(datum)

        image = self.load_region_image(datum, regions)     
        if save_image:  # debugging purpose...
            image.save('region.jpg')
        image = self.vis_processor(image)   

        if 'full_text_input' in datum:
            text_input = self.parse_region_tokens(datum['full_text_input'], regions)
        else:
            question = self.parse_region_tokens(datum['question'], regions)
            answer = self.parse_region_tokens(datum['answer'], regions)
            rationale = self.parse_region_tokens(datum['rationale'], regions)

            if random.random() > 0.5:
                text_input = "{} Answer: {} Rationale: {}".format(question, answer, rationale)
            else:
                text_input = "{} Answer: {}".format(question, answer)
        text_input = self.text_processor(text_input) 

        return {
                "image": image, 
                "text_input": text_input,
                'instance_id': datum['index'],
                "image_id": index
            }
    
    def collater(self, samples):
        image_list, text_input, instance_id_list, image_id_list = [], [], [], []

        for sample in samples:
            image_list.append(sample["image"])
            text_input.append(sample["text_input"])
            instance_id_list.append(sample["instance_id"])
            image_id_list.append(sample["image_id"])

        return {
            "image": torch.stack(image_list, dim=0),
            "text_input": text_input,
            "instance_id": instance_id_list,
            "image_id": torch.tensor(image_id_list, dtype=torch.int),
        }

class UnifiedVQAChatGPTContrastiveEvalDataset(UnifiedVQAChatGPTEvalDataset):
    """ 
        UnifiedVQA Contrastive Training for Data Generated by ChatGPT.
        This is used to train BLIP2 Model.
    """

    def __init__(self, vis_processor, text_processor, vis_root, ann_paths, info):        
        super().__init__(vis_processor, text_processor, vis_root, ann_paths, info)

    def __getitem__(self, index):
        datum = self.annotation[index]

        regions = self.get_regions(datum)
        
        image = self.load_region_image(datum, regions)     
        image = self.vis_processor(image)   

        if 'full_text_input' in datum:
            text_input = self.parse_region_tokens(datum['full_text_input'], regions)
        else:
            question = self.parse_region_tokens(datum['question'], regions)
            answer = self.parse_region_tokens(datum['answer'], regions)
            rationale = self.parse_region_tokens(datum['rationale'], regions)
            text_input = "{} Answer: {} Rationale: {}".format(question, answer, rationale)
        text_input = self.text_processor(text_input) 

        return {
                "image": image, 
                "text_input": text_input,
                'instance_id': datum['index'],
                "image_id": index
            }
    
    def collater(self, samples):
        image_list, text_input, instance_id_list, image_id_list = [], [], [], []

        for sample in samples:
            image_list.append(sample["image"])
            text_input.append(sample["text_input"])
            instance_id_list.append(sample["instance_id"])
            image_id_list.append(sample["image_id"])

        return {
            "image": torch.stack(image_list, dim=0),
            "text_input": text_input,
            "instance_id": instance_id_list,
            "image_id": torch.tensor(image_id_list, dtype=torch.int),
        }

# if __name__ == "__main__":
#     from lavis.datasets.datasets.unified_vqa_chatgpt_datasets import UnifiedVQAChatGPTContrastiveDataset
#     from lavis.processors import load_processor
#     UnifiedVQAChatGPTContrastiveDataset(
#         vis_processor=load_processor('blip_image_train'),
#         text_processor=
#     )